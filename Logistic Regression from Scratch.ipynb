{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e125bbd",
      "metadata": {
        "tags": [],
        "id": "4e125bbd"
      },
      "source": [
        "### In this exercise I will attempt to build a Logistic Regression Estimator from the ground up.\n",
        "\n",
        "Initially, I use a Base class as a foundation and I build functionality on each step of the inheritance process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "48c995fd",
      "metadata": {
        "id": "48c995fd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4eba3743",
      "metadata": {
        "id": "4eba3743"
      },
      "outputs": [],
      "source": [
        "class BaseClassifier:\n",
        "    def __init__(self, theta=0.1, alpha=0.1, max_it=10, pred_threshold=0.5):\n",
        "        self.theta = theta\n",
        "        self.alpha = alpha\n",
        "        self.max_it = max_it\n",
        "        self.pred_threshold = pred_threshold\n",
        "        self.name = \"Binary Classifier\"\n",
        "    \n",
        "    @classmethod\n",
        "    def from_list(self, param_list):\n",
        "        a = BaseClassifier()\n",
        "        if (len(param_list) == 4):\n",
        "            a.theta, a.alpha, a.max_it, a.pred_threshold = param_list\n",
        "            return a\n",
        "        else:\n",
        "            raise Exception('Length is incorrect!')\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return \"Hi I am a \" + self.name\n",
        "    \n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.predict(*args, **kwargs)\n",
        "    \n",
        "    def predict(self, *args, **kwargs):\n",
        "        assert not hasattr(super(), 'predict')\n",
        "    \n",
        "    def train(self, *args, **kwargs):\n",
        "        assert not hasattr(super(), 'train')\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8cd3f4c1",
      "metadata": {
        "id": "8cd3f4c1",
        "outputId": "3d3fce1b-6f44-400e-ecba-4d79b8435b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6de0383c012e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbase_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbase_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-d8359051bc16>\u001b[0m in \u001b[0;36mfrom_list\u001b[0;34m(self, param_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length is incorrect!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Length is incorrect!"
          ]
        }
      ],
      "source": [
        "# This should print 1 and the appropriate error message\n",
        "\n",
        "base_clf = BaseClassifier.from_list([1,1,1,1])\n",
        "print(base_clf.theta)\n",
        "base_clf = BaseClassifier.from_list([1,1,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4a8d4f",
      "metadata": {
        "tags": [],
        "id": "bc4a8d4f"
      },
      "source": [
        "I need data to train the model on. I define class 1 to be Normally distributed around +2 and  class 0 to be normally distributed around -2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "82923d5a",
      "metadata": {
        "id": "82923d5a"
      },
      "outputs": [],
      "source": [
        "x1 = np.random.randn(100,2) + 2\n",
        "x2 = np.random.randn(100,2) - 2\n",
        "X = np.concatenate([x1,x2], axis=0)\n",
        "y  = np.concatenate([np.ones(100), np.zeros(100)], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98986aa3",
      "metadata": {
        "tags": [],
        "id": "98986aa3"
      },
      "source": [
        "I define a class LogisticRegression that inherits from the previously defined BaseClassifier which is a binary classifier based on the sigmoid neuron. In this section, I want to implement the __init__ funtion:\n",
        "* to be able to have all the functionality of the BaseClassifier\n",
        "* change the name of the class to \"Logistic Regression Classifier\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "570b974f",
      "metadata": {
        "id": "570b974f"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(BaseClassifier):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.name = 'LogisticRegressionClassifier'\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1.0 / (1 + np.exp(-z))\n",
        "    \n",
        "    def cost_fcn(self, x, y):\n",
        "        z = np.dot(x, self.theta)\n",
        "        y_pred = self.sigmoid(z)\n",
        "        cost = - np.dot(y, np.log(y_pred)) - np.dot((1-y), np.log(1-y_pred))\n",
        "        return cost\n",
        "\n",
        "    def gradients(self, x, y):\n",
        "        z = np.dot(x, self.theta)\n",
        "        h = self.sigmoid(z)\n",
        "        return (1.0 /self.train_size) * np.dot(x.T, (h-y))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        pred = np.dot(X, self.theta)\n",
        "        pred[pred >= 0.5] = 1\n",
        "        pred[pred < 0.5] = 0\n",
        "        return pred\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        cost = []\n",
        "        self.theta = np.random.rand(X.shape[1])\n",
        "        self.train_size = X.shape[0]\n",
        "        for it in range(self.max_it): \n",
        "            cost.append(self.cost_fcn(X,y))\n",
        "            grads = self.gradients(X, y)\n",
        "            self.theta = self.theta - self.alpha * grads\n",
        "        print(\"Cost Function per iteration:\")\n",
        "        print(cost)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e81e82bf",
      "metadata": {
        "id": "e81e82bf",
        "outputId": "bee3dbfe-ae48-43cd-97f0-17ac441f84ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi I am a LogisticRegressionClassifier\n",
            "Model alpha:  0.01\n"
          ]
        }
      ],
      "source": [
        "clf1 = LogisticRegression(alpha=0.01)\n",
        "print(clf1)\n",
        "print(\"Model alpha: \", clf1.alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fc3972eb",
      "metadata": {
        "id": "fc3972eb",
        "outputId": "6efd1fba-f77b-49af-f1bf-aa49d258ba59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost Function per iteration:\n",
            "[28.7935103246232, 28.608019567485318, 28.42526347048498, 28.245181257001285, 28.06771393039164, 27.89280420988623, 27.720396469203145, 27.550436677751875, 27.382872344300324, 27.21765246298712]\n",
            "Training accuracy:  1.0\n"
          ]
        }
      ],
      "source": [
        "clf1.train(X,y)\n",
        "print(\"Training accuracy: \", sum(clf1.predict(X))/(X.shape[0]/2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6dee53f",
      "metadata": {
        "tags": [],
        "id": "d6dee53f"
      },
      "source": [
        "It is very common to have data points close to the decision boundary to have observation errors or be mislabeled. Below,I will introducing mislabeledslabeled datapoints to the training dataset. I add 10 additional datapoints and end up with 210 training datapoints in general and both are again type of np.array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ea841be4",
      "metadata": {
        "id": "ea841be4"
      },
      "outputs": [],
      "source": [
        "x1_misslabel = np.random.randn(5,2) + 1\n",
        "x2_misslabel = np.random.randn(5,2) - 1\n",
        "X_misslabel = np.concatenate([x1_misslabel,x2_misslabel], axis=0)    \n",
        "y_misslabel  = np.concatenate([np.zeros(5), np.ones(5)], axis=0)\n",
        "X_prime = np.concatenate([X, X_misslabel], axis=0)\n",
        "y_prime = np.concatenate([y, y_misslabel], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963f279d",
      "metadata": {
        "tags": [],
        "id": "963f279d"
      },
      "source": [
        "In order to be able to handle mislabeled training data points and avoid overfitting I wish to leverage regularization. Below I define a class that extends the LogisticRegression class and updates its functions to incorporate regularization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "89d92754",
      "metadata": {
        "id": "89d92754"
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionRegularized(LogisticRegression):\n",
        "    def __init__(self, regularization_coef=0.01, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.regularization_coef = regularization_coef\n",
        "        \n",
        "    def cost_fcn(self, x, y):\n",
        "        cost = super().cost_fcn(x, y)\n",
        "        cost += self.regularization_coef/(2* x.shape[0])*(np.dot(self.theta,self.theta))\n",
        "        return cost\n",
        "    \n",
        "    def gradients(self, x, y):\n",
        "        gradient = super().gradients(x, y)\n",
        "        gradient += (1.0 /self.train_size)*self.regularization_coef*self.theta\n",
        "        return gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "63bcd0f9",
      "metadata": {
        "id": "63bcd0f9"
      },
      "outputs": [],
      "source": [
        "clf2 = LogisticRegressionRegularized(max_it=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "774eeaec",
      "metadata": {
        "id": "774eeaec",
        "outputId": "60b9a394-fa2f-43ec-fbce-1c04b951a4e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost Function per iteration:\n",
            "[102.45323571695855, 84.3341347188418, 72.57681806534605, 64.47258124017553, 58.60146832691392, 54.176574447252946, 50.73502987408694, 47.98957208451409, 45.75353729691718, 43.90081243453098]\n",
            "Training accuracy:  0.9809523809523809\n"
          ]
        }
      ],
      "source": [
        "clf2.train(X_prime, y_prime)\n",
        "print(\"Training accuracy: \", sum(clf2.predict(X_prime))/(X_prime.shape[0]/2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1a281f",
      "metadata": {
        "id": "6a1a281f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "kubeflow_notebook": {
      "autosnapshot": false,
      "docker_image": "",
      "experiment": {
        "id": "",
        "name": ""
      },
      "experiment_name": "",
      "katib_metadata": {
        "algorithm": {
          "algorithmName": "grid"
        },
        "maxFailedTrialCount": 3,
        "maxTrialCount": 12,
        "objective": {
          "objectiveMetricName": "",
          "type": "minimize"
        },
        "parallelTrialCount": 3,
        "parameters": []
      },
      "katib_run": false,
      "pipeline_description": "",
      "pipeline_name": "",
      "snapshot_volumes": false,
      "steps_defaults": [],
      "volume_access_mode": "rwm",
      "volumes": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}