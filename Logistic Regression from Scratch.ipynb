{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e125bbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In this exercise we will attempt to build a Logistic Regression Estimator from the ground up.\n",
    "\n",
    "Initially we use a Base class as a foundation and we build functionality on each step of the inheritance process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c995fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2f6c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "In the BaseClassifier class below complete the classmethod from_list. The method is going to help create instances of the class from a python list. Specifically we need the method to:\n",
    "* have one position argument named \"param_list\" of type list.\n",
    "* assign each element of the \"param_list\" to an instance initialization argument in sequence.\n",
    "* throw an exception if there are missing arguments or more arguments than expected with an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eba3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassifier:\n",
    "    def __init__(self, theta=0.1, alpha=0.1, max_it=10, pred_threshold=0.5):\n",
    "        self.theta = theta\n",
    "        self.alpha = alpha\n",
    "        self.max_it = max_it\n",
    "        self.pred_threshold = pred_threshold\n",
    "        self.name = \"Binary Classifier\"\n",
    "    \n",
    "    # Here you need to implement your code.\n",
    "    @classmethod\n",
    "    def from_list(self, param_list):\n",
    "        a = BaseClassifier()\n",
    "        if (len(param_list) == 4):\n",
    "            a.theta, a.alpha, a.max_it, a.pred_threshold = param_list\n",
    "            return a\n",
    "        else:\n",
    "            raise Exception('Length is incorrect!')\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Hi I am a \" + self.name\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.predict(*args, **kwargs)\n",
    "    \n",
    "    def predict(self, *args, **kwargs):\n",
    "        assert not hasattr(super(), 'predict')\n",
    "    \n",
    "    def train(self, *args, **kwargs):\n",
    "        assert not hasattr(super(), 'train')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd3f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Length is incorrect!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m base_clf \u001b[38;5;241m=\u001b[39m BaseClassifier\u001b[38;5;241m.\u001b[39mfrom_list([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(base_clf\u001b[38;5;241m.\u001b[39mtheta)\n\u001b[0;32m----> 5\u001b[0m base_clf \u001b[38;5;241m=\u001b[39m \u001b[43mBaseClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mBaseClassifier.from_list\u001b[0;34m(self, param_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength is incorrect!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Length is incorrect!"
     ]
    }
   ],
   "source": [
    "# This should print 1 and the appropriate error message\n",
    "\n",
    "base_clf = BaseClassifier.from_list([1,1,1,1])\n",
    "print(base_clf.theta)\n",
    "base_clf = BaseClassifier.from_list([1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a8d4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We need data to train our model on. We define our class 1 to be Normally distributed around +2 and our class 0 to be normally distributed around -2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82923d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.randn(100,2) + 2\n",
    "x2 = np.random.randn(100,2) - 2\n",
    "X = np.concatenate([x1,x2], axis=0)\n",
    "y  = np.concatenate([np.ones(100), np.zeros(100)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98986aa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "We define a class LogisticRegression that inherits from the previously defined BaseClassifier which is a binary classifier based on the sigmoid neuron. In this exercise we are looking to implement the __init__ funtion. Specifically we want:\n",
    "* to be able to have all the functionality of the BaseClassifier\n",
    "* change the name of the class to \"Logistic Regression Classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d0041",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "During the training step of the gradient descent algorithm we need to calculate the value of the cost function. Since this is a binary classifier we can leverage the binary cross entropy as our cost function. The goal of this exercise is the development of the method \"cost_fcn\". Specifically we want to:\n",
    "* take 2 arguments that have been declared below x and y. x is type np.array(train_size, 2) and y is type np.array(train_size,).\n",
    "* calculate the binary cross entropy of the sigmoid output and true label \"J\" as scalar.\n",
    "* return the scalar \"J\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570b974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(BaseClassifier):\n",
    "    # Here you are going to implement the answer for Exercise 2\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.name = 'LogisticRegressionClassifier'\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Here you are going to implement the answer for Exercise 3\n",
    "    def cost_fcn(self, x, y):\n",
    "        z = np.dot(x, self.theta)\n",
    "        y_pred = self.sigmoid(z)\n",
    "        cost = - np.dot(y, np.log(y_pred)) - np.dot((1-y), np.log(1-y_pred))\n",
    "        return cost\n",
    "\n",
    "    def gradients(self, x, y):\n",
    "        h = self.sigmoid(np.dot(x, self.theta))\n",
    "        return (1.0 /self.train_size) * np.dot(x.T, (h-y))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = np.dot(X, self.theta)\n",
    "        pred[pred >= 0.5] = 1\n",
    "        pred[pred < 0.5] = 0\n",
    "        return pred\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        cost = []\n",
    "        self.theta = np.random.rand(X.shape[1])\n",
    "        self.train_size = X.shape[0]\n",
    "        for it in range(self.max_it): \n",
    "            cost.append(self.cost_fcn(X,y))\n",
    "            grads = self.gradients(X, y)\n",
    "            self.theta = self.theta - self.alpha * grads\n",
    "        print(\"Cost Function per iteration:\")\n",
    "        print(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81e82bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi I am a LogisticRegressionClassifier\n",
      "Model alpha:  0.01\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(alpha=0.01)\n",
    "print(clf1)\n",
    "print(\"Model alpha: \", clf1.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3972eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Function per iteration:\n",
      "[26.279754786609033, 26.144236576520335, 26.01037713090243, 25.878145976349096, 25.74751337713285, 25.61845031321458, 25.490928459027366, 25.36492016300317, 25.240398427812515, 25.117336891288762]\n",
      "Training accuracy:  0.99\n"
     ]
    }
   ],
   "source": [
    "clf1.train(X,y)\n",
    "print(\"Training accuracy: \", sum(clf1.predict(X))/(X.shape[0]/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dee53f",
   "metadata": {
    "tags": []
   },
   "source": [
    "It is very common to have data points close to the decision boundary to have observation errors or being misslabeled. Below we are introducing to our training dataset misslabeled datapoints. We add 10 additional datapoints and end up with 210 training datapoints in general and both are again type of np.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea841be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_misslabel = np.random.randn(5,2) + 1\n",
    "x2_misslabel = np.random.randn(5,2) - 1\n",
    "X_misslabel = np.concatenate([x1_misslabel,x2_misslabel], axis=0)    \n",
    "y_misslabel  = np.concatenate([np.zeros(5), np.ones(5)], axis=0)\n",
    "X_prime = np.concatenate([X, X_misslabel], axis=0)\n",
    "y_prime = np.concatenate([y, y_misslabel], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f279d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "In order to be able to handle misslabeled training data points and avoid overfitting we wish to leverage regularization. Below we define a class that extends the LogisticRegression class and updates its functions to incorporate regularization. \n",
    "\n",
    "### Part 1\n",
    "We want to update the current \"cost_fcn\" with the appropriate regularization. \n",
    "* We are expanding the inherited function and we are keeping the same number of arguments and types, x: np.array(train_size, 2) and y: np.array(train_size,)\n",
    "* Return a scalar that incorporates the regularization value\n",
    "(tip: you can use the np.linalg library)\n",
    "\n",
    "### Part 2\n",
    "We want to update the current \"gradients\" method to reflect the change in the \"cost_fcn\". Specifically:\n",
    "* We are expanding the inherited function and we are keeping the same number of arguments and types, x: np.array(train_size, 2) and y: np.array(train_size,)\n",
    "* Return an np.array(2,) that contains the gradients for \"theta: based on the updated \"cost_fnc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d92754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionRegularized(LogisticRegression):\n",
    "    # Part 0: set a good initial value for regularization_coef\n",
    "    def __init__(self, regularization_coef=0.1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.regularization_coef = regularization_coef\n",
    "        \n",
    "    def cost_fcn(self, x, y):\n",
    "        cost = super().cost_fcn(x, y)\n",
    "        cost += self.regularization_coef/(2* x.shape[0])*(np.dot(self.theta,self.theta))\n",
    "        return cost\n",
    "    \n",
    "    def gradients(self, x, y):\n",
    "        h = self.sigmoid(np.dot(x, self.theta))\n",
    "        return (1.0 /self.train_size) * np.dot(x.T, (h-y)) + (1.0 /self.train_size)*self.regularization_coef*self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63bcd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus 0: do you think other arguments like max_it play role in avoiding the current problem?\n",
    "clf2 = LogisticRegressionRegularized(max_it=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774eeaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Function per iteration:\n",
      "[138.53778998659521, 108.99922683468404, 90.16869337332128, 77.58911189136103, 68.75737362913752, 62.28267198350173, 57.363560598055315, 53.515906260043444, 50.43360185832649, 47.915056615062305]\n",
      "Training accuracy:  0.9619047619047619\n"
     ]
    }
   ],
   "source": [
    "clf2.train(X_prime, y_prime)\n",
    "print(\"Training accuracy: \", sum(clf2.predict(X_prime))/(X_prime.shape[0]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512aa7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "",
   "experiment": {
    "id": "",
    "name": ""
   },
   "experiment_name": "",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
